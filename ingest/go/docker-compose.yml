version: "3.8"

services:
  influxdb:
    image: influxdb:2
    container_name: influxdb
    restart: always
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME_FILE: /run/secrets/influxdb-admin-username
      DOCKER_INFLUXDB_INIT_PASSWORD_FILE: /run/secrets/influxdb-admin-password
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE: /run/secrets/influxdb-admin-token
      DOCKER_INFLUXDB_INIT_ORG: myorg
      DOCKER_INFLUXDB_INIT_BUCKET: telemetry
    secrets:
      - influxdb-admin-username
      - influxdb-admin-password
      - influxdb-admin-token
    volumes:
      - influxdb2-data:/var/lib/influxdb2
      - influxdb2-config:/etc/influxdb2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          cpus: ".6"
          memory: 1G

  go_app:
    build: .
    container_name: go-telemetry
    restart: "no"
    depends_on:
      influxdb:
        condition: service_healthy
    networks:
      - telemetry-network
    environment:
      INFLUXDB_URL: http://influxdb:8086
      INFLUXDB_ORG: myorg
      INFLUXDB_BUCKET: telemetry
      INFLUXDB_TOKEN_FILE: /run/secrets/influxdb-admin-token
      # Performance tuning
      MAX_BATCH_SIZE: "5000"
      BATCH_TIMEOUT: "1s"
      MAX_RETRIES: "3"
      RETRY_DELAY: "500ms"
      PARALLEL_GROUPS: "1"
      BATCH_SIZE: "3000"
      # Golang performance tuning
      GOMAXPROCS: "0" # Let Go determine based on available CPUs
      GOGC: "100" # Default garbage collection threshold
    secrets:
      - influxdb-admin-token
    volumes:
      - ./ibt_files:/app/ibt_files
    entrypoint:
      [
        "/app/telemetry-app",
        "/app/ibt_files/mclaren720sgt3_monza full 2025-02-09 12-58-11.ibt",
      ]
    deploy:
      resources:
        limits:
          cpus: ".6"
          memory: 1G

  # Kafka broker with KRaft mode enabled (combined role)
  kafka-1:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-1
    hostname: kafka-1
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_KRAFT_MODE: "true"
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:9094
      KAFKA_LISTENERS: PLAINTEXT://kafka-1:9093,EXTERNAL://0.0.0.0:9092,CONTROLLER://kafka-1:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9093,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # Large message settings
      KAFKA_MESSAGE_MAX_BYTES: 409715200 # 200MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 409715200 # 200MB
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 209715200
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 209715200
      KAFKA_FETCH_MAX_BYTES: 209715200
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_COMPRESSION_TYPE: lz4
      # Performance settings
      KAFKA_NUM_PARTITIONS: 8
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_RETENTION_HOURS: 72
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000 # 5 minutes
      # KRaft settings
      KAFKA_METADATA_LOG_DIR: /var/lib/kafka/kraft-metadata
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: "EWLmw6qURnOrFSdphNRUbw"
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./data:/var/lib/kafka/data
      - /var/run/docker.sock:/var/run/docker.sock
      - kafka-1-metadata:/var/lib/kafka/kraft-metadata
    command:
      - sh
      - -c
      - |
        echo "Creating Kafka config properties file..."
        cat > /tmp/server.properties << EOF
        node.id=1
        process.roles=broker,controller
        controller.quorum.voters=1@kafka-1:9094
        listeners=PLAINTEXT://kafka-1:9093,EXTERNAL://0.0.0.0:9092,CONTROLLER://kafka-1:9094
        advertised.listeners=PLAINTEXT://kafka-1:9093,EXTERNAL://localhost:9092
        listener.security.protocol.map=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
        inter.broker.listener.name=PLAINTEXT
        controller.listener.names=CONTROLLER
        EOF

        # Ensure proper permissions on directories
        mkdir -p /var/lib/kafka/data /var/lib/kafka/kraft-metadata
        chown -R appuser:appuser /var/lib/kafka/data /var/lib/kafka/kraft-metadata
        chmod -R 775 /var/lib/kafka/data /var/lib/kafka/kraft-metadata

        if [ ! -f /var/lib/kafka/kraft-metadata/meta.properties ]; then
          echo "Formatting storage for Kafka..."
          kafka-storage format -t ${CLUSTER_ID} -c /tmp/server.properties
        fi

        exec /etc/confluent/docker/run
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"
    healthcheck:
      test:
        [
          "CMD",
          "bash",
          "-c",
          "kafka-topics --bootstrap-server=kafka-1:9093 --list || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 90s

  # Kafka Setup for topic creation
  kafka-setup:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-setup
    depends_on:
      kafka-1:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command: |
      echo "Starting Kafka topic creation script..."

      # Retry mechanism
      MAX_RETRIES=30
      RETRY_INTERVAL=10

      for i in $(seq 1 $MAX_RETRIES); do
        echo "Attempt $$i of $$MAX_RETRIES: Checking if Kafka is ready..."

        if kafka-topics --list --bootstrap-server=kafka-1:9093 > /dev/null 2>&1; then
          echo "Kafka is ready! Creating topic..."

          kafka-topics --create --if-not-exists \
            --bootstrap-server=kafka-1:9093 \
            --partitions=3 \
            --replication-factor=1 \
            --config=max.message.bytes=400000000 \
            --config=retention.ms=604800000 \
            --config=segment.bytes=1073741824 \
            --topic=large-files

          if [ $? -eq 0 ]; then
            echo "Topic 'large-files' created or already exists!"
            exit 0
          else
            echo "Failed to create topic. Error code: $?"
          fi

          break
        else
          echo "Kafka not yet ready. Waiting $$RETRY_INTERVAL seconds..."
          sleep $RETRY_INTERVAL
        fi
      done

      echo "Maximum retries reached. Could not connect to Kafka!"
      exit 1
    restart: on-failure:3
    networks:
      - telemetry-network

  # Schema Registry service
  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    container_name: schema-registry
    depends_on:
      kafka-1:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "kafka-1:9093"
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    volumes:
      - schema-registry-data:/var/lib/schema-registry
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

  # Kafka Connect service
  kafka-connect:
    image: confluentinc/cp-kafka-connect:latest
    container_name: kafka-connect
    depends_on:
      kafka-1:
        condition: service_healthy
      schema-registry:
        condition: service_started
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka-1:9093"
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.converters.ByteArrayConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      # For handling large files
      CONNECT_MAX_REQUEST_SIZE: 409715200
      # Plugin path
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

  # Kafka UI for management and monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      kafka-1:
        condition: service_healthy
      schema-registry:
        condition: service_started
      kafka-connect:
        condition: service_started
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:9093
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: kafka-connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

secrets:
  influxdb-admin-username:
    file: ./.env.influxdb-admin-username
  influxdb-admin-password:
    file: ./.env.influxdb-admin-password
  influxdb-admin-token:
    file: ./.env.influxdb-admin-token

volumes:
  influxdb2-data:
  influxdb2-config:
  # Kafka volumes created with appropriate permissions
  kafka-1-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/kafka-data
      o: bind
  kafka-1-metadata:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/kafka-metadata
      o: bind
  schema-registry-data:

networks:
  telemetry-network:
    external: true
    name: telemetry-network
