version: "3.8"

services:
  influxdb:
    image: influxdb:2
    container_name: influxdb
    restart: always
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME_FILE: /run/secrets/influxdb-admin-username
      DOCKER_INFLUXDB_INIT_PASSWORD_FILE: /run/secrets/influxdb-admin-password
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE: /run/secrets/influxdb-admin-token
      DOCKER_INFLUXDB_INIT_ORG: myorg
      DOCKER_INFLUXDB_INIT_BUCKET: telemetry
    secrets:
      - influxdb-admin-username
      - influxdb-admin-password
      - influxdb-admin-token
    volumes:
      - influxdb2-data:/var/lib/influxdb2
      - influxdb2-config:/etc/influxdb2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          cpus: ".6"
          memory: 1G

  go_app:
    build: .
    container_name: go-telemetry
    restart: "no"
    depends_on:
      influxdb:
        condition: service_healthy
    networks:
      - telemetry-network
    environment:
      INFLUXDB_URL: http://influxdb:8086
      INFLUXDB_ORG: myorg
      INFLUXDB_BUCKET: telemetry
      INFLUXDB_TOKEN_FILE: /run/secrets/influxdb-admin-token
      # Performance tuning
      MAX_BATCH_SIZE: "5000"
      BATCH_TIMEOUT: "1s"
      MAX_RETRIES: "3"
      RETRY_DELAY: "500ms"
      PARALLEL_GROUPS: "1"
      BATCH_SIZE: "3000"
      # Golang performance tuning
      GOMAXPROCS: "0" # Let Go determine based on available CPUs
      GOGC: "100" # Default garbage collection threshold
    secrets:
      - influxdb-admin-token
    volumes:
      - ./ibt_files:/app/ibt_files
    entrypoint:
      [
        "/app/telemetry-app",
        "/app/ibt_files/mclaren720sgt3_monza full 2025-02-09 12-58-11.ibt",
      ]
    deploy:
      resources:
        limits:
          cpus: ".6"
          memory: 1G
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 10
      ZOOKEEPER_SYNC_LIMIT: 5
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: "srvr,mntr,ruok,stat"
    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Kafka Broker 1
  kafka-1:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9093,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Large message settings
      KAFKA_MESSAGE_MAX_BYTES: 409715200 # 200MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 409715200 # 200MB
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 209715200
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 209715200
      KAFKA_FETCH_MAX_BYTES: 209715200
      KAFKA_COMPRESSION_TYPE: lz4
      # Performance settings
      KAFKA_NUM_PARTITIONS: 8
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168 # 1 week
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000 # 5 minutes
      # JVM settings
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "1.5"

    healthcheck:
      test:
        ["CMD", "kafka-topics", "--bootstrap-server", "kafka-1:9093", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Kafka Broker 2
  kafka-2:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-2
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9093,EXTERNAL://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Large message settings
      KAFKA_MESSAGE_MAX_BYTES: 409715200 # 200MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 409715200 # 200MB
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 209715200
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 209715200
      KAFKA_FETCH_MAX_BYTES: 209715200
      KAFKA_COMPRESSION_TYPE: lz4
      # Performance settings
      KAFKA_NUM_PARTITIONS: 8
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168 # 1 week
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000 # 5 minutes
      # JVM settings
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
    volumes:
      - kafka-2-data:/var/lib/kafka/data
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "1.5"

  # Kafka Broker 3
  kafka-3:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-3
    depends_on:
      - zookeeper
    ports:
      - "9095:9095"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9093,EXTERNAL://localhost:9095
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Large message settings
      KAFKA_MESSAGE_MAX_BYTES: 409715200 # 200MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 409715200 # 200MB
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 209715200
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 209715200
      KAFKA_FETCH_MAX_BYTES: 209715200
      KAFKA_COMPRESSION_TYPE: lz4
      # Performance settings
      KAFKA_NUM_PARTITIONS: 8
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168 # 1 week
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000 # 5 minutes
      # JVM settings
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
    volumes:
      - kafka-3-data:/var/lib/kafka/data
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "1.5"

  kafka-setup:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-setup
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    entrypoint: ["/bin/sh", "-c"]
    command: |
      echo "Starting Kafka topic creation script..."

      # Retry mechanism
      MAX_RETRIES=30
      RETRY_INTERVAL=10

      for i in $(seq 1 $MAX_RETRIES); do
        echo "Attempt $$i of $$MAX_RETRIES: Checking if Kafka is ready..."

        if kafka-topics --list --bootstrap-server=kafka-1:9093 > /dev/null 2>&1; then
          echo "Kafka is ready! Creating topic..."

          kafka-topics --create --if-not-exists \
            --bootstrap-server=kafka-1:9093 \
            --partitions=8 \
            --replication-factor=3 \
            --config=max.message.bytes=400000000 \
            --config=retention.ms=604800000 \
            --config=segment.bytes=1073741824 \
            --topic=large-files

          if [ $? -eq 0 ]; then
            echo "Topic 'large-files' created or already exists!"
            exit 0
          else
            echo "Failed to create topic. Error code: $?"
          fi

          break
        else
          echo "Kafka not yet ready. Waiting $$RETRY_INTERVAL seconds..."
          sleep $RETRY_INTERVAL
        fi
      done

      echo "Maximum retries reached. Could not connect to Kafka!"
      exit 1
    restart: on-failure:3
    networks:
      - telemetry-network

  # Schema Registry service
  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    container_name: schema-registry
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "kafka-1:9093,kafka-2:9093,kafka-3:9093"
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    volumes:
      - schema-registry-data:/var/lib/schema-registry
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

  # Kafka Connect service
  kafka-connect:
    image: confluentinc/cp-kafka-connect:latest
    container_name: kafka-connect
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka-1:9093,kafka-2:9093,kafka-3:9093"
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.converters.ByteArrayConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      # For handling large files
      CONNECT_MAX_REQUEST_SIZE: 409715200
      # Plugin path
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

  # Kafka UI for management and monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry
      - kafka-connect
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:9093,kafka-2:9093,kafka-3:9093
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: kafka-connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
    networks:
      - telemetry-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

secrets:
  influxdb-admin-username:
    file: ./.env.influxdb-admin-username
  influxdb-admin-password:
    file: ./.env.influxdb-admin-password
  influxdb-admin-token:
    file: ./.env.influxdb-admin-token

volumes:
  influxdb2-data:
  influxdb2-config:
  zookeeper-data:
  zookeeper-logs:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
  schema-registry-data:

networks:
  telemetry-network:
    external: true
    name: telemetry-network
